{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capestone Project Solution2_2\n",
    "## / Toxic Comment Classification /\n",
    "\n",
    "- - -\n",
    "<ul>\n",
    "<li><a href=\"#prepare\">I 环境准备</a></li>\n",
    "<li><a href=\"#vecing\">II 向量化</a></li>\n",
    "<li><a href=\"#model\">III 模型</a></li>\n",
    "<li><a href=\"#conclusions\">IV 结论</a></li>\n",
    "</ul>\n",
    "\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id='prepare'>I 环境准备</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 导入语句\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 导入机器学习库\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 导入深度学习库\n",
    "# from sklearn.datasets import load_files       \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "#from keras.layers import Flatten\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import Dropout\n",
    "from keras.layers import Conv1D, MaxPool1D, BatchNormalization\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# 设置参数显示长文本\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "# 行内显示\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0  0000997932d777bf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入文件\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# 检查标签\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comment\n",
    "train_comment = train.comment_text\n",
    "test_comment = test.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lable\n",
    "train_label = train.iloc[:,2:]\n",
    "train_label.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "(159571,)\n",
      "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\n",
      "(153164,)\n",
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "print(train_comment[0])\n",
    "print(train_comment.shape)\n",
    "print(test_comment[0])\n",
    "print(test_comment.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id='vecing'>II 向量化</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6B 300D ---\n",
    "# 根据 solution2_test 文件，300D的成绩明显好于50D\n",
    "# 也说明合适的维度非常重要\n",
    "\n",
    "# 向量化词\n",
    "\n",
    "max_feature = 10000\n",
    "max_lenth = 100\n",
    "\n",
    "## prepare tokenizer\n",
    "t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', num_words=max_feature )\n",
    "## https://keras.io/preprocessing/text/\n",
    "## 可以考虑直接去掉奇怪字符、lower\n",
    "t.fit_on_texts(train_comment)\n",
    "## 这里是对输入句子进行拆词\n",
    "## 在这个数据里，如果输入了1000句（测试文件中的train_short)，结果是10007个\n",
    "## 如果输出的话在1000之后会有个...\n",
    "## 全部输入的话是21万多\n",
    "## 可以使用 num_words 做限制（这里没有生效，后续研究）\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "## 根据上面设置 vocab_size\n",
    "\n",
    "## integer encode the documents\n",
    "encoded_train = t.texts_to_sequences(train_comment)\n",
    "encoded_test = t.texts_to_sequences(test_comment)\n",
    "\n",
    "## pad documents to a max length of 100 words\n",
    "max_length = 300\n",
    "\n",
    "## max_length 就是每个commet要处理的单词数\n",
    "padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 300)\n",
      "[   6   40   33   42  173  282  145   89   26   22    6   96    5  672\n",
      "  884   16  528    2   33   57    4   18   57 2292    8   39   73  371\n",
      "    4    6  361    2   16  101 1463   21  567   37    6  292   18    5\n",
      "  672   50   96   48   11 1018  439 3246    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(153164, 300)\n",
      "[ 104    6   33    5    3    1  342  363    3   50    5  445  352    3\n",
      "  536   31 1343  310   15    1  734    7   72   77   48  262   63   26\n",
      "    7   33  214    9    1  342    6   18  659  104   16 1325  334    2\n",
      " 1414    1 1257    3   13   23   57  468  104   16  659   10    5 3639\n",
      "   28   23   22  841    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# check padded\n",
    "print(padded_train.shape)\n",
    "print(padded_train[99])\n",
    "\n",
    "print(padded_test.shape)\n",
    "print(padded_test[100])\n",
    "## 可以看出来，100个词的 padded 数据，没有的用0填充了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210337"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check word list\n",
    "len(t.word_index)\n",
    "## 注意这里是先将词统计完的计数\n",
    "## 与 max_length 无关，因为 pad_sequences 是后发生的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id='model'>III 模型</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 创建 embedding 层（300D）\n",
    "\n",
    "embed_demention = 300\n",
    "\n",
    "## 导入pre-training\n",
    "embeddings_index = dict( )\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "## 创建嵌入矩阵\n",
    "embedding_matrix = np.zeros((vocab_size, embed_demention))\n",
    "## 注意这里要和嵌入矩阵的维度300相同\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210338, 300)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check embedding_matrix\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all class split 测试\n",
    "\n",
    "# 使用 pretrained embedding \n",
    "# eporch 会影响准确度\n",
    "# eporch5 和 eporch20 \n",
    "\n",
    "# split train and val\n",
    "x_train, x_val , y_train, y_val = train_test_split(padded_train, train_label, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 300, 300)          63101400  \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 300, 300)          1200      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 300, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 150, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 150, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 57606     \n",
      "=================================================================\n",
      "Total params: 63,281,230\n",
      "Trainable params: 179,102\n",
      "Non-trainable params: 63,102,128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## model\n",
    "model = Sequential()\n",
    "## 设置embedding层\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
    "model.add(e)\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64, 5, padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 3, padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(MaxPool1D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "# 6分类 Dense为6\n",
    "\n",
    "## compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "## summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/5\n",
      " - 2396s - loss: 0.1664 - acc: 0.9730 - val_loss: 0.2794 - val_acc: 0.8944\n",
      "Epoch 2/5\n",
      " - 2255s - loss: 0.1004 - acc: 0.9753 - val_loss: 0.0910 - val_acc: 0.9758\n",
      "Epoch 3/5\n",
      " - 2255s - loss: 0.0904 - acc: 0.9758 - val_loss: 0.0864 - val_acc: 0.9755\n",
      "Epoch 4/5\n",
      " - 2256s - loss: 0.0840 - acc: 0.9760 - val_loss: 0.0822 - val_acc: 0.9762\n",
      "Epoch 5/5\n",
      " - 2257s - loss: 0.0824 - acc: 0.9764 - val_loss: 0.0842 - val_acc: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a494f4a58>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fit the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a442ca7f31cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's2_weight.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_weights('s2_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tip\n",
    "### （在最开始的测试中）Tokenizer不做任何处理是 Accuracy: 95.334365\n",
    "### 加了处理结果有一点上升 Accuracy: 95.417087\n",
    "### 具有随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id='conclusions'>IV 结论</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output submission\n",
    "y_target = model.predict(padded_test)\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_target\n",
    "submission.to_csv('submission_s2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 主要参考资料：\n",
    "1. [项目建议中的LR + 词袋模式](https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams)\n",
    "2. [Cross-validation Performance](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)\n",
    "\n",
    "> 小结：\n",
    "1. Solution1 为 LR + CBOW 的方式进行多分类计算\n",
    "2. 输出结果是每个分类的可能性[0,1]\n",
    "\n",
    "> Kaggle Score:\n",
    "1. 0.97576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
