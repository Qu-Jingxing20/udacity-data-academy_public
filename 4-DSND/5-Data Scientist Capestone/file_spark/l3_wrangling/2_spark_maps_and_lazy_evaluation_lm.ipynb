{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:17:43.030201Z",
     "start_time": "2019-10-31T00:17:42.886245Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e91d34768ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/findspark.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[1;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:19:00.952479Z",
     "start_time": "2019-10-31T00:19:00.526194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_212\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_212-b10)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "# spark env check（mac）\n",
    "!java -version\n",
    "# 如果不是 1.8 需要\n",
    "## 关掉jupyter notebook，之后在terminal运行\n",
    "## export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_212`\n",
    "## 如果版本号不对（小版本可能会升级）需要查询 /usr/libexec/java_home -V\n",
    "## 完成后再次检查版本，输出为类似 java version \"1.8.0_212\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps\n",
    "\n",
    "In Spark, maps take data as input and then transform that data with whatever function you put in the map. They are like directions for the data telling how each input should get to the output.\n",
    "\n",
    "The first code cell creates a SparkContext object. With the SparkContext, you can input a dataset and parallelize the data across a cluster (since you are currently using Spark in local mode on a single machine, technically the dataset isn't distributed yet).\n",
    "\n",
    "Run the code cell below to instantiate a SparkContext object and then read in the log_of_songs list into Spark. \n",
    "\n",
    "在 Spark 中，map会把输入数据按照map中的函数进行转换它们就像数据的指引，告诉每个输入要如何到达输出 。\n",
    "\n",
    "第一个单元创建一个 SparkContext 对象。你可以使用 SparkContext 把输入数据分布到集群中（因为你当前使用的是单机模式，严格讲数据集没有分布到集群上）。\n",
    "\n",
    "运行下面的代码单元来实例化 SparkContext 对象，然后用Spark 读取 log_of_songs 列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:19:13.230872Z",
     "start_time": "2019-10-31T00:19:07.294874Z"
    }
   },
   "outputs": [],
   "source": [
    "### \n",
    "# You might have noticed this code in the screencast.\n",
    "#\n",
    "# import findspark\n",
    "# findspark.init('spark-2.3.2-bin-hadoop2.7')\n",
    "#\n",
    "# The findspark Python module makes it easier to install\n",
    "# Spark in local mode on your computer. This is convenient\n",
    "# for practicing Spark syntax locally. \n",
    "# However, the workspaces already have Spark installed and you do not\n",
    "# need to use the findspark module\n",
    "#\n",
    "###\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"maps_and_lazy_evaluation_example\")\n",
    "\n",
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "# parallelize the log_of_songs to use with Spark\n",
    "distributed_song_log = sc.parallelize(log_of_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code cell defines a function that converts a song title to lowercase. Then there is an example converting the word \"Havana\" to \"havana\".\n",
    "\n",
    "下一个代码单元定义了一个将歌曲标题转换为小写的函数。有一个例子将 “Havana\"  改写成  “havana\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:19:15.836503Z",
     "start_time": "2019-10-31T00:19:15.815797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'havana'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "convert_song_to_lowercase(\"Havana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cells demonstrate how to apply this function using a map step. The map step will go through each song in the list and apply the convert_song_to_lowercase() function. \n",
    "\n",
    "以下代码单元演示了如何通过 map 来应用此函数。map 步骤会遍历列表中的每首歌曲并且把convert_song_to_lowercase() 函数应用在歌曲上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:19:20.988019Z",
     "start_time": "2019-10-31T00:19:20.933307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(convert_song_to_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this code cell ran quite quickly. This is because of lazy evaluation. Spark does not actually execute the map step unless it needs to.\n",
    "\n",
    "\"RDD\" in the output refers to resilient distributed dataset. RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data. \n",
    "\n",
    "To get Spark to actually run the map step, you need to use an \"action\". One available action is the collect method. The collect() method takes the results from all of the clusters and \"collects\" them into a single list on the master node.\n",
    "\n",
    "这个代码单元运行得很快。这是惰性评估的原因。不必要的情况下 Spark 不会执行 map 步骤。\n",
    "\n",
    "输出中的 “RDD” 指的是弹性分布式数据集。RDD 正是他们所说的：分布在集群中的容错数据集。这就是 Spark 存储数据的方式。 \n",
    "\n",
    "要让 Spark 运行 map 步骤，你需要使用一个“工具”。有个可用的“工具”叫 collect 方法。collect() 方法会把所有集群中结果收集到主节点上的单个列表中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T03:28:55.796312Z",
     "start_time": "2019-10-08T03:28:55.791515Z"
    }
   },
   "source": [
    "**基于MacJava8环境切换:**\n",
    "1. java版本切换\n",
    "https://www.java.com/zh_CN/download/ 官网下载 apk\n",
    "201910月看到的是 Version8 Update221 (20190716)\n",
    "2. 检查版本\n",
    "```\n",
    "$ /usr/libexec/java_home -V\n",
    "Matching Java Virtual Machines (2):\n",
    "    12.0.1, x86_64:\t\"Java SE 12.0.1\"\t/Library/Java/JavaVirtualMachines/jdk-12.0.1.jdk/Contents/Home\n",
    "    1.8.0_212, x86_64:\t\"Java SE 8\"\t/Library/Java/JavaVirtualMachines/jdk1.8.0_212.jdk/Contents/Home\n",
    "```\n",
    "3. 切换版本\n",
    "```\n",
    "$ export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_212`\n",
    "```\n",
    "4. 检查版本\n",
    "```\n",
    "$ java -version\n",
    "java version \"1.8.0_212\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_212-b10)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)\n",
    "```\n",
    "5. 注意事项\n",
    "- 貌似默认还是java12的, 需要在一个终端中做完上述操作后,再运行conda环境 和 notebook\n",
    "- https://kodejava.org/how-do-i-set-the-default-java-jdk-version-on-mac-os-x/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:20:16.843625Z",
     "start_time": "2019-10-31T00:20:16.726690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(convert_song_to_lowercase).collect()\n",
    "# java version 不是8的话会报错,需要降级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:20:20.369790Z",
     "start_time": "2019-10-31T00:20:20.359430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark 和 python 的时间处理\n",
    "## 上述代码第一次run 2.7s, 后续为92ms\n",
    "## 这可能和 spark 开始对数据的初始化工作有关\n",
    "## 对比下纯 python 的方法 10ms:\n",
    "## 感觉初始化之后的时间spark很慢, 可能和数据集大小有关\n",
    "new_list = []\n",
    "for song in log_of_songs:\n",
    "    new_list.append(song.lower())\n",
    "    \n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as well that Spark is not changing the original data set: Spark is merely making a copy. You can see this by running collect() on the original dataset.\n",
    "\n",
    "另外，请注意，Spark 不会更改原始数据集：Spark 只是复制了一份副本。你可以在原始数据集上运行collect() 来确认这点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:20:31.275222Z",
     "start_time": "2019-10-31T00:20:31.208854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despacito',\n",
       " 'Nice for what',\n",
       " 'No tears left to cry',\n",
       " 'Despacito',\n",
       " 'Havana',\n",
       " 'In my feelings',\n",
       " 'Nice for what',\n",
       " 'despacito',\n",
       " 'All the stars']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not always have to write a custom function for the map step. You can also use anonymous (lambda) functions as well as built-in Python functions like string.lower(). \n",
    "\n",
    "Anonymous functions are actually a Python feature for writing functional style programs.\n",
    "\n",
    "在 map 步骤，有时你不用编写自定义函数。你还可以使用匿名（lambda）函数以及string.lower() 等内置Python函数。 \n",
    "\n",
    "匿名函数实际上是一个Python特性，专门用于编写函数式程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T00:20:36.632409Z",
     "start_time": "2019-10-31T00:20:36.532055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(lambda x: x.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
