{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Env\n",
    "\n",
    "本文件代码来自 tutorialspoint 的 Quick Guide 内容：https://www.tutorialspoint.com/pyspark/pyspark_quick_guide.htm\n",
    "\n",
    "- 涉及到的文件需要规整一下比较方便（后续完成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T07:50:51.507342Z",
     "start_time": "2019-10-28T07:50:46.768283Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"First App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T07:52:23.888841Z",
     "start_time": "2019-10-28T07:52:23.648264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_212\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_212-b10)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T07:55:39.374060Z",
     "start_time": "2019-10-28T07:55:39.144145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 672, lines with b: 347\n"
     ]
    }
   ],
   "source": [
    "logFile = \"test_README.md\"\n",
    "logData = sc.textFile(logFile).cache()\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print (\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T07:56:32.986086Z",
     "start_time": "2019-10-28T07:56:32.957458Z"
    }
   },
   "outputs": [],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T07:57:21.663461Z",
     "start_time": "2019-10-28T07:57:21.555627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "# count()\n",
    "counts = words.count()\n",
    "print(\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T07:59:38.438057Z",
     "start_time": "2019-10-28T07:59:38.384480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "# collect()\n",
    "coll = words.collect()\n",
    "print(\"Elements in RDD -> %s\" % (coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:03:50.025062Z",
     "start_time": "2019-10-28T08:03:49.966173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# foreach(f)\n",
    "def f(x): print(x)\n",
    "fore = words.foreach(f) \n",
    "print(\"%s\" % (fore))\n",
    "# 这里没有输出，例子中是输出每一个元素\n",
    "# 版本相差较多，后续研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:03:10.838512Z",
     "start_time": "2019-10-28T08:03:10.731849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "# filter(f)\n",
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD -> %s\" % (filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:04:36.916662Z",
     "start_time": "2019-10-28T08:04:36.847288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "# map(f)\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair -> %s\" % (mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:09:16.561556Z",
     "start_time": "2019-10-28T08:09:14.794472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "# reduce(f)\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "\n",
    "# 遇到报错 Cannot run multiple SparkContexts at once 是因为前面有 sc 还在运行呢\n",
    "# 使用 sc.stop() \n",
    "sc.stop()\n",
    "\n",
    "sc = SparkContext(\"local\", \"Reduce app\")\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print(\"Adding all the elements -> %i\" % (adding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:11:41.150157Z",
     "start_time": "2019-10-28T08:11:39.220690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# join(other)\n",
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Join app\")\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print(\"Join RDD -> %s\" % (final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:12:53.579831Z",
     "start_time": "2019-10-28T08:12:53.305119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got chached > True\n"
     ]
    }
   ],
   "source": [
    "# cache()\n",
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Cache app\") \n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(\"Words got chached > %s\" % (caching))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast / Accumulator\n",
    "For parallel processing, Apache Spark uses shared variables. A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, so that it can be used for performing tasks.\n",
    "\n",
    "Broadcast variables are used to save the copy of data across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:18:53.016845Z",
     "start_time": "2019-10-28T08:18:52.735341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored data -> ['scala', 'java', 'hadoop', 'spark', 'akka']\n",
      "Printing a particular element in RDD -> hadoop\n"
     ]
    }
   ],
   "source": [
    "# broadcast()\n",
    "sc.stop()\n",
    "\n",
    "sc = SparkContext(\"local\", \"Broadcast app\") \n",
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
    "data = words_new.value \n",
    "print(\"Stored data -> %s\" % (data)) \n",
    "elem = words_new.value[2] \n",
    "print(\"Printing a particular element in RDD -> %s\" % (elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:21:17.579552Z",
     "start_time": "2019-10-28T08:21:16.052496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated value is -> 150\n"
     ]
    }
   ],
   "source": [
    "# Accumulator\n",
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"Accumulator app\") \n",
    "num = sc.accumulator(10) \n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num.value \n",
    "print(\"Accumulated value is -> %i\" % (final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkConf\n",
    "\n",
    "To run a Spark application on the local/cluster, you need to set a few configurations and parameters, this is what SparkConf helps with. It provides configurations to run a Spark application. The following code block has the details of a SparkConf class for PySpark.\n",
    "\n",
    "- Attributes\n",
    "    - set(key, value) − To set a configuration property.\n",
    "    - setMaster(value) − To set the master URL.\n",
    "    - setAppName(value) − To set an application name.\n",
    "    - get(key, defaultValue=None) − To get a configuration value of a key.\n",
    "    - setSparkHome(value) − To set Spark installation path on worker nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:30:41.433817Z",
     "start_time": "2019-10-28T08:30:41.295750Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "#conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
    "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"local\")\n",
    "## set URL 会卡住，后续EMR中测试\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkFiles\n",
    "\n",
    "In Apache Spark, you can upload your files using **sc.addFile** (sc is your default SparkContext) and get the path on a worker using **SparkFiles.get**. Thus, SparkFiles resolve the paths to files added through **SparkContext.addFile()**.\n",
    "\n",
    "SparkFiles contain the following classmethods −\n",
    "\n",
    "* get(filename)\n",
    "* getrootdirectory()\n",
    "\n",
    "\n",
    "## StorageLevel\n",
    "\n",
    "\n",
    "StorageLevel decides how RDD should be stored. In Apache Spark, StorageLevel decides whether RDD should be stored in the memory or should it be stored over the disk, or both. It also decides whether to serialize RDD and whether to replicate RDD partitions.\n",
    "\n",
    "```python\n",
    "class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib\n",
    "\n",
    "Apache Spark offers a Machine Learning API called **MLlib**. PySpark has this machine learning API in Python as well. It supports different kind of algorithms, which are mentioned below −\n",
    "\n",
    "* **mllib.classification** − The **spark.mllib** package supports various methods for binary classification, multiclass classification and regression analysis. Some of the most popular algorithms in classification are **Random Forest, Naive Bayes, Decision Tree**, etc.\n",
    "\n",
    "* **mllib.clustering** − Clustering is an unsupervised learning problem, whereby you aim to group subsets of entities with one another based on some notion of similarity.\n",
    "\n",
    "* **mllib.fpm** − Frequent pattern matching is mining frequent items, itemsets, subsequences or other substructures that are usually among the first steps to analyze a large-scale dataset. This has been an active research topic in data mining for years.\n",
    "\n",
    "* **mllib.linalg** − MLlib utilities for linear algebra.\n",
    "\n",
    "* **mllib.recommendation** − Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user item association matrix.\n",
    "\n",
    "* **spark.mllib** − It ¬currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.mllib uses the Alternating Least Squares (ALS) algorithm to learn these latent factors.\n",
    "\n",
    "* **mllib.regression** − Linear regression belongs to the family of regression algorithms. The goal of regression is to find relationships and dependencies between variables. The interface for working with linear regression models and model summaries is similar to the logistic regression case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:42:39.485733Z",
     "start_time": "2019-10-28T08:42:21.748522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 4.443558894358244e-06\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   sc = SparkContext(appName=\"Pspark mllib Example\")\n",
    "   data = sc.textFile(\"test.data\")\n",
    "   ratings = data.map(lambda l: l.split(','))\\\n",
    "      .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "   \n",
    "   # Build the recommendation model using Alternating Least Squares\n",
    "   rank = 10\n",
    "   numIterations = 10\n",
    "   model = ALS.train(ratings, rank, numIterations)\n",
    "   \n",
    "   # Evaluate the model on training data\n",
    "   testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "   predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "   ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "   MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "   print(\"Mean Squared Error = \" + str(MSE))\n",
    "   \n",
    "   # Save and load model\n",
    "   model.save(sc, \"target/tmp/myCollaborativeFilter\")\n",
    "   sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializers\n",
    "\n",
    "Serialization is used for performance tuning on Apache Spark. All data that is sent over the network or written to the disk or persisted in the memory should be serialized. Serialization plays an important role in costly operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T08:45:47.675375Z",
     "start_time": "2019-10-28T08:45:45.549528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.serializers import MarshalSerializer\n",
    "sc = SparkContext(\"local\", \"serialization app\", serializer = MarshalSerializer())\n",
    "print(sc.parallelize(list(range(1000))).map(lambda x: 2 * x).take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
