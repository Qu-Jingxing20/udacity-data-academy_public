{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Load and Clean Dataset](#preprocess)\n",
    "    1. [Import necessary packages](#package)\n",
    "    2. [Preprocessing walkthrough](#walkthrough)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "    1. [Define churn](#churn)\n",
    "    2. [Explore data](#exdata)\n",
    "4. [Feature Engineering](#feature_eng)\n",
    "5. [Modeling](#modeling)\n",
    "    1. [Metrics](#metrics)\n",
    "    2. [Baseline Model](#baseline)\n",
    "    3. [Logistic Regression](#lr)\n",
    "    4. [Gradient Boosted Trees](#gbt)\n",
    "    5. [Support Vector Machine](#svm)\n",
    "    6. [Random Forest](#rf)\n",
    "    7. [Model Summary](#model_summary)\n",
    "    8. [Hyper Params Tuning](#tuning)\n",
    "    9. [Best Model](#best_model)\n",
    "    10. [Best Model Metrics](#final_metrics)\n",
    "    11. [Feature Importance from GBTClassifier](#importance)\n",
    "6. [Conclusion](#conclusion)\n",
    "    1. [Summary](#conclusion)\n",
    "    2. [Reflection](#conclusion)\n",
    "    3. [Improvement](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id='overview'></a>\n",
    "Sparkify is a music streaming service just as Spotify and Pandora. \n",
    "\n",
    "The data provided is the user log of the service, having demographic info, user activities, timestamps and etc. We try to analyze the log and build a model to identify customers who are highly likely to quit using our service, and thus, send marketing offers to them to prevent them from churning. \n",
    "\n",
    "We use F1 score to measure of model performance because we need precision and recall at the same time as we don't want to miss too many customers who are likely to churn whilst we don't want to waste too much on those who are not likely to churn. \n",
    "\n",
    "We performed EDA to establish features, and train several machine learning classification models.\n",
    "\n",
    "The model we built and selected has a F1 score of 0.800, which is 16% higher than sending everybody offers. There is also a short article about this project posted here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem that I will attempt to solve will be to predict customer churn in advance and I will be using spark (PySpark) and the Gradient boosting models classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Dataset <a id='preprocess'></a>\n",
    "In this workspace, the filename is `medium-sparkify-event-data.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The chunk below is the starter code for using IBM-Watson cloud. I've already anonymized the credentials and config. IBM-Watson cloud will automatically generate this chunk of code once you upload your data from local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages <a id='package'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T01:04:48.608023Z",
     "start_time": "2019-10-16T01:04:46.239735Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark related libraries\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier,LinearSVC, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# import pandas, matplotlib, and seaborn for visualization and EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import datetime for parsing datetime object\n",
    "import datetime\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T01:07:10.595572Z",
     "start_time": "2019-10-16T01:07:05.908508Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "#sc =SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T01:07:14.748564Z",
     "start_time": "2019-10-16T01:07:11.902921Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o24.json.\n: java.io.IOException: No FileSystem for scheme: cos\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-01069f114ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf_data_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'medium-sparkify-event-data.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'your_project_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdf_data_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.json.\n: java.io.IOException: No FileSystem for scheme: cos\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'some_url',\n",
    "    'api_key': 'you_api_key',\n",
    "    'service_id': 'your_service_id',\n",
    "    'iam_service_endpoint': 'your_token'}\n",
    "\n",
    "configuration_name = 'os_a68347975ac949879ccd31e5ca85693c_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df_data_1 = spark.read.json(cos.url('medium-sparkify-event-data.json', 'your_project_name'))\n",
    "df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing walk through <a id='walkthrough'></a>\n",
    "1. Load the data from json file.\n",
    "2. Delete NAs.\n",
    "3. Remove rows with empty as `UserId`.\n",
    "4. Convert `gender` column to binary numeric column.\n",
    "5. Convert `ts` and `registration` columns to human readable time stamp format.\n",
    "6. Create `churn` column, 1 representing user has churned, 0 otherwise. If event `Cancellation Confirmation` observed, we assume the user has churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we clean our dataset, checking for invalid or missing data. For example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.935467Z",
     "start_time": "2019-10-15T15:13:18.713Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop NAs\n",
    "df_valid = df_data_1.dropna(how = 'any', subset = ['userId', 'sessionId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping missing values, we found that missing userId is actually represented by an empty string as shown below. A record with empty userId is probably generated when the user didn't log in or havn't signed up yet, and it contributes little to our analysis afterwards. Therefore, we drop these rows with empty string as userId. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.938571Z",
     "start_time": "2019-10-15T15:13:18.718Z"
    }
   },
   "outputs": [],
   "source": [
    "# inspect 'userId' column\n",
    "df_valid.select('userId').dropDuplicates().sort('userId').show(5)\n",
    "# drop empty strings\n",
    "df_valid = df_valid.filter(df_valid['userId'] != '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a id='eda'></a>\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse of the data when are about to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.942509Z",
     "start_time": "2019-10-15T15:13:18.726Z"
    }
   },
   "outputs": [],
   "source": [
    "df_valid_pd = df_valid.toPandas()\n",
    "df_valid_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn <a id='churn'></a>\n",
    "\n",
    "We defined a user as `churned` when there's an event called `Cancellation Confirmation` appeared in activity log. This event happens for both paid and free users, meaning that their account associated with us was terminated at the time. When also kept in track of a downgrade event, labeled as `Submit Downgrade` in activity log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.946524Z",
     "start_time": "2019-10-15T15:13:18.732Z"
    }
   },
   "outputs": [],
   "source": [
    "# create downgrade label\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "df_valid = df_valid.withColumn(\"downgrade_event\", flag_downgrade_event(\"page\"))\n",
    "\n",
    "# label user who've ever downgraded\n",
    "windowval = Window.partitionBy('userId')\n",
    "df_valid = df_valid.withColumn('downgrade', max('downgrade_event').over(windowval))\n",
    "\n",
    "# create churn label\n",
    "flag_churn_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "df_valid = df_valid.withColumn('churn_event', flag_churn_event('page'))\n",
    "\n",
    "# label user who churned\n",
    "windowval = Window.partitionBy('userId')\n",
    "df_valid = df_valid.withColumn('churn', max('churn_event').over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.949658Z",
     "start_time": "2019-10-15T15:13:18.737Z"
    }
   },
   "outputs": [],
   "source": [
    "df_valid.select(['userId', 'churn', 'downgrade']).dropDuplicates().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the table above, the time stamp columns aren't quite intuitive for human to read, so we put a little effort to convert these columns to a more human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.953617Z",
     "start_time": "2019-10-15T15:13:18.742Z"
    }
   },
   "outputs": [],
   "source": [
    "# add a time column to df\n",
    "convert_ts = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "df_valid = df_valid.withColumn('event_time', convert_ts('ts'))\n",
    "df_valid = df_valid.withColumn('registration_time', convert_ts('registration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data <a id='exdata'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.957039Z",
     "start_time": "2019-10-15T15:13:18.749Z"
    }
   },
   "outputs": [],
   "source": [
    "df_valid.select('page').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly 19 activities that a user may have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer lifetime statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.960648Z",
     "start_time": "2019-10-15T15:13:18.754Z"
    }
   },
   "outputs": [],
   "source": [
    "lifetime_pd = df_valid.where('page == \"NextSong\" OR page == \"Thumbs Up\"').groupby(['userId', 'churn', 'gender', 'page']).count().toPandas()\n",
    "lifetime_pd = lifetime_pd.pivot_table(index=['userId','churn','gender'], values='count', columns='page').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.963773Z",
     "start_time": "2019-10-15T15:13:18.759Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(data=lifetime_pd, x='churn', y='NextSong', hue='gender')\n",
    "plt.xlabel('Has customer churned?')\n",
    "plt.ylabel('Lifetime songs listened')\n",
    "plt.legend(title='Gender', loc='best')\n",
    "plt.title('Customer lifetime songs listened didn\\'t vary a lot')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.968165Z",
     "start_time": "2019-10-15T15:13:18.763Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(data=lifetime_pd, x='churn', y='Thumbs Up', hue='gender')\n",
    "plt.xlabel('Has customer churned?')\n",
    "plt.ylabel('Lifetime songs liked')\n",
    "plt.legend(title='Gender', loc='best')\n",
    "plt.title('Customer lifetime songs liked didn\\'t vary a lot')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Churn pattern between genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.970999Z",
     "start_time": "2019-10-15T15:13:18.769Z"
    }
   },
   "outputs": [],
   "source": [
    "gender_churn_pd = df_valid.dropDuplicates([\"userId\", \"gender\"]).groupby([\"churn\", \"gender\"]).count().sort(\"churn\").toPandas()\n",
    "\n",
    "ax = sns.barplot(x='churn', y='count', hue='gender', data=gender_churn_pd)\n",
    "plt.xlabel('Has customer churned?')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender', loc='best')\n",
    "plt.title('Male customers are slightly likely to churn')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of songs played per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.974605Z",
     "start_time": "2019-10-15T15:13:18.774Z"
    }
   },
   "outputs": [],
   "source": [
    "songs_gender_pd = df_valid.where('page == \"NextSong\"').groupby(['churn', 'userId', 'sessionId','gender']).count()\\\n",
    "    .groupby(['churn', 'userId', 'gender']).agg({'count':'avg'})\\\n",
    "    .withColumnRenamed('avg(count)', 'avg_songs_played')\\\n",
    "    .toPandas()\n",
    "\n",
    "ax = sns.violinplot('churn', y='avg_songs_played', hue='gender', data=songs_gender_pd)\n",
    "plt.xlabel('Has customer churned?')\n",
    "plt.ylabel('Average songs played per session')\n",
    "plt.legend(title='Gender', loc='best', ncol=2)\n",
    "plt.title('Churned customers played less music per session')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At which level do customers churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.977684Z",
     "start_time": "2019-10-15T15:13:18.779Z"
    }
   },
   "outputs": [],
   "source": [
    "level_pd = df_valid.filter('page == \"Cancellation Confirmation\"').groupby('level').count().toPandas()\n",
    "\n",
    "ax = sns.barplot(data=level_pd, x='level', y='count', color=sns.color_palette()[0])\n",
    "plt.xlabel('User level when churn')\n",
    "plt.ylabel('')\n",
    "plt.title('Churn usually happens when a customer is using a paid plan')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time since registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.980349Z",
     "start_time": "2019-10-15T15:13:18.784Z"
    }
   },
   "outputs": [],
   "source": [
    "lt_pd = df_valid \\\n",
    "    .select('userId','registration','ts','churn') \\\n",
    "    .withColumn('lifetime',(df_valid.ts-df_valid.registration)) \\\n",
    "    .groupBy('userId','churn') \\\n",
    "    .agg({'lifetime':'max'}) \\\n",
    "    .withColumnRenamed('max(lifetime)','lifetime') \\\n",
    "    .select('userId', 'churn', (col('lifetime')/1000/3600/24).alias('lifetime')) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.983964Z",
     "start_time": "2019-10-15T15:13:18.789Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=lt_pd, y='churn', x='lifetime', orient='h')\n",
    "plt.xlabel('Days since registration when churn')\n",
    "plt.ylabel('Has customer churned')\n",
    "plt.title('Churned customer use the service for a shorter period of time')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering <a id='feature_eng'></a>\n",
    "We've familiarized ourselves with the data, we begin to build out the features we find promising to train your model on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first feature we considered is the time length since the user registered. It may reflect the user engagement, loyality, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.988121Z",
     "start_time": "2019-10-15T15:13:18.796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time since registration\n",
    "f1 = df_valid \\\n",
    "    .select('userId','registration','ts') \\\n",
    "    .withColumn('lifetime',(df_valid.ts-df_valid.registration)) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg({'lifetime':'max'}) \\\n",
    "    .withColumnRenamed('max(lifetime)','lifetime') \\\n",
    "    .select('userId', (col('lifetime')/1000/3600/24).alias('lifetime'))\n",
    "f1.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The second feature we considered is the total number of songs listened. The more songs the user listened, the more time the user spent with our service, the deeper engagement the user have, leading to less chance to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.992711Z",
     "start_time": "2019-10-15T15:13:18.802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total songs listened\n",
    "f2 = df_valid \\\n",
    "    .select('userID','song') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'total_songs')\n",
    "f2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The third and fourth features are number of thumbs up and down. They may reflect two perspectives, our service quality and user engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.995737Z",
     "start_time": "2019-10-15T15:13:18.807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of Thumbs-Up/Down\n",
    "f3 = df_valid \\\n",
    "    .select('userID','page') \\\n",
    "    .where(df_valid.page == 'Thumbs Up') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'num_thumb_up') \n",
    "f3.describe().show()\n",
    "f4 = df_valid \\\n",
    "    .select('userID','page') \\\n",
    "    .where(df_valid.page == 'Thumbs Down') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'num_thumb_down')\n",
    "f4.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The fifth feature is the number of songs added to the playlist, and it functioned as about the same as the 3rd and 4th feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:18.999866Z",
     "start_time": "2019-10-15T15:13:18.813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of songs added to playlist\n",
    "f5 = df_valid \\\n",
    "    .select('userID','page') \\\n",
    "    .where(df_valid.page == 'Add to Playlist') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'add_to_playlist')\n",
    "f5.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The sixth feature is the number of friends added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.002826Z",
     "start_time": "2019-10-15T15:13:18.820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of friends added\n",
    "f6 = df_valid \\\n",
    "    .select('userID','page') \\\n",
    "    .where(df_valid.page == 'Add Friend') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'add_friend') \n",
    "f6.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The seventh feature is the length of listening, serving the same objective as the total number of songs listened, but in a time perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.005823Z",
     "start_time": "2019-10-15T15:13:18.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total length of listening\n",
    "f7 = df_valid \\\n",
    "    .select('userID','length') \\\n",
    "    .groupBy('userID') \\\n",
    "    .sum() \\\n",
    "    .withColumnRenamed('sum(length)', 'listen_time')\n",
    "f7.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The eighth feature is the average number of songs listened per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.009860Z",
     "start_time": "2019-10-15T15:13:18.839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of songs listened per session\n",
    "f8 = df_valid.where('page == \"NextSong\"') \\\n",
    "    .groupby(['userId', 'sessionId']) \\\n",
    "    .count() \\\n",
    "    .groupby(['userId']) \\\n",
    "    .agg({'count':'avg'}) \\\n",
    "    .withColumnRenamed('avg(count)', 'avg_songs_played')\n",
    "f8.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ninth feature is the gender, as we want to include some demographic information about the user, and there are differences between genders in terms of chance to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.013404Z",
     "start_time": "2019-10-15T15:13:18.845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gender\n",
    "f9 = df_valid \\\n",
    "    .select(\"userId\", \"gender\") \\\n",
    "    .dropDuplicates() \\\n",
    "    .replace(['M', 'F'], ['0', '1'], 'gender') \\\n",
    "    .select('userId', col('gender').cast('int'))\n",
    "f9.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The tenth feature is the number of artists listened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.016223Z",
     "start_time": "2019-10-15T15:13:18.849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of artists listened\n",
    "f10 = df_valid \\\n",
    "    .filter(df_valid.page==\"NextSong\") \\\n",
    "    .select(\"userId\", \"artist\") \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupby(\"userId\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"artist_count\")\n",
    "f10.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At last, we have the churn label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.020750Z",
     "start_time": "2019-10-15T15:13:18.854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Churn label\n",
    "label = df_valid \\\n",
    "    .select('userId', col('churn').alias('label')) \\\n",
    "    .dropDuplicates()\n",
    "label.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we merge all features and label together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.024277Z",
     "start_time": "2019-10-15T15:13:18.859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct \n",
    "data = f1.join(f2,'userID','outer') \\\n",
    "    .join(f3,'userID','outer') \\\n",
    "    .join(f4,'userID','outer') \\\n",
    "    .join(f5,'userID','outer') \\\n",
    "    .join(f6,'userID','outer') \\\n",
    "    .join(f7,'userID','outer') \\\n",
    "    .join(f8,'userID','outer') \\\n",
    "    .join(f9,'userID','outer') \\\n",
    "    .join(f10,'userID','outer') \\\n",
    "    .join(label,'userID','outer') \\\n",
    "    .drop('userID') \\\n",
    "    .fillna(0)\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics <a id='metrics'></a>\n",
    "We select F-1 score as our evaluation metrics. The reason we use F-1 score here is because it gives us a simple measure of the precision (whether we send offer to the right person) and recall (whether we miss one that we should’ve sent the offer) of the model. We want to identify those who are likely to churn and give them some special offers in trying to keep the customer, but at the same time, we do not want to send too many offers (most likely a monetary incentive) to those who are not as likely to churn and therefore wasting money and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to vectorize our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.027312Z",
     "start_time": "2019-10-15T15:13:18.866Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vector assembler\n",
    "cols = [\"lifetime\", \"total_songs\", \"num_thumb_up\", \\\n",
    "          'num_thumb_down','add_to_playlist','add_friend','listen_time','avg_songs_played', \\\n",
    "        'gender', 'artist_count']\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"NumFeatures\")\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid one feature with larger scale dominant the whole model, we standardize the features by taking off the mean and divided by the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.030157Z",
     "start_time": "2019-10-15T15:13:18.870Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"features\", withStd=True)\n",
    "scalerModel = scaler.fit(data)\n",
    "data = scalerModel.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data into train, validation, and test sets by calling `randomSplit` method twice. We can also conduct a stratified sampling as our label is pretty skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.033572Z",
     "start_time": "2019-10-15T15:13:18.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train, rest = data.randomSplit([0.6, 0.4], seed=42)\n",
    "validation, test = rest.randomSplit([0.5, 0.5], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model <a id='baseline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate two baseline models, one with all users labelled as `churn = 1`, one with all users labelled as `churn = 0`. And we calculated the model accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.036280Z",
     "start_time": "2019-10-15T15:13:18.884Z"
    }
   },
   "outputs": [],
   "source": [
    "results_base_all_1 = test.withColumn('prediction', lit(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.039011Z",
     "start_time": "2019-10-15T15:13:18.888Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Test set metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_base_all_1, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_base_all_1, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.043329Z",
     "start_time": "2019-10-15T15:13:18.893Z"
    }
   },
   "outputs": [],
   "source": [
    "results_base_all_0 = test.withColumn('prediction', lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.046317Z",
     "start_time": "2019-10-15T15:13:18.898Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Test set metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_base_all_0, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_base_all_0, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the baseline model of labelling all users with `churn = 0` actually does a descent job on test set, with accuracy of 79.2% and f1 score of 0.6996."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we evaluate four different models, using cross validation to minimize the chance of overfitting, and grid search to tune our model. After evaluating the four models on validation set, we pick the best one based on the f1 score on that and train the best model on training set again then to evaluate on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression <a id='lr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.049113Z",
     "start_time": "2019-10-15T15:13:18.904Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# set evaluator\n",
    "f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "# build paramGrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=lr,\n",
    "                          evaluator=f1_evaluator, \n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.052158Z",
     "start_time": "2019-10-15T15:13:18.910Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "cvModel_lr = crossval_lr.fit(train)\n",
    "end = time()\n",
    "cvModel_lr.avgMetrics\n",
    "print('The training process took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.055168Z",
     "start_time": "2019-10-15T15:13:18.914Z"
    }
   },
   "outputs": [],
   "source": [
    "results_lr = cvModel_lr.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.171756Z",
     "start_time": "2019-10-15T15:13:19.158346Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MulticlassClassificationEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f47fc7ad4b6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logistic Regression Metrics:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F-1 Score:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MulticlassClassificationEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Logistic Regression Metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_lr, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_lr, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees <a id='gbt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.176723Z",
     "start_time": "2019-10-15T15:13:18.927Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "gbt = GBTClassifier(maxIter=10,seed=42)\n",
    "\n",
    "# set evaluator\n",
    "f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "# build paramGrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "crossval_gbt = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.180175Z",
     "start_time": "2019-10-15T15:13:18.932Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "cvModel_gbt = crossval_gbt.fit(train)\n",
    "end = time()\n",
    "cvModel_gbt.avgMetrics\n",
    "print('The training process took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.184699Z",
     "start_time": "2019-10-15T15:13:18.937Z"
    }
   },
   "outputs": [],
   "source": [
    "results_gbt = cvModel_gbt.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.187925Z",
     "start_time": "2019-10-15T15:13:18.941Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Gradient Boosted Trees Metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_gbt, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_gbt, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine <a id='svm'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.191554Z",
     "start_time": "2019-10-15T15:13:18.947Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "svm = LinearSVC(maxIter=10)\n",
    "\n",
    "# set evaluator\n",
    "f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "# build paramGrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "crossval_svm = CrossValidator(estimator=svm,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.194556Z",
     "start_time": "2019-10-15T15:13:18.952Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "cvModel_svm = crossval_svm.fit(train)\n",
    "end = time()\n",
    "cvModel_svm.avgMetrics\n",
    "print('The training process took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.197691Z",
     "start_time": "2019-10-15T15:13:18.957Z"
    }
   },
   "outputs": [],
   "source": [
    "results_svm = cvModel_svm.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.200659Z",
     "start_time": "2019-10-15T15:13:18.962Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('SVM Metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_svm, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_svm, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest <a id='rf'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.203727Z",
     "start_time": "2019-10-15T15:13:18.967Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# set evaluator\n",
    "f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "# build paramGrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.206317Z",
     "start_time": "2019-10-15T15:13:18.972Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "cvModel_rf = crossval_rf.fit(train)\n",
    "end = time()\n",
    "cvModel_rf.avgMetrics\n",
    "print('The training process took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.208636Z",
     "start_time": "2019-10-15T15:13:18.977Z"
    }
   },
   "outputs": [],
   "source": [
    "results_rf = cvModel_rf.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.211426Z",
     "start_time": "2019-10-15T15:13:18.982Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Random Forest Metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_rf, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_rf, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model summary: <a id='model_summary'></a>\n",
    "* The logistic regression model has a accuracy of: 0.735, and F1 score of:0.622, using 1047.5 seconds on our server.\n",
    "* The gradient boosted trees model has a accuracy of: 0.776, and F1 score of: 0.756, using 1560.5 seconds on our server.\n",
    "* The support vector machine model has a accuracy of: 0.735, and F1 score of: 0.622, using 1184.0 seconds on our server.\n",
    "* The random forest model has a accuracy of: 0.776, and F1 score of: 0.708, using 1220.6 seconds on our server.\n",
    "\n",
    "\n",
    "\n",
    "Although we do care about time resources, but since the data size is still reletively small, and the performance difference is huge, we will prefer the model that perform the best. Therefore, we choose GBT model as our final used model and conduct a grid search to fine tune our model this time. \n",
    "\n",
    "In the future, we may instead implement random forest model for more time efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams tunning <a id='tuning'></a>\n",
    "\n",
    "We use grid search to fine tune our GBT Classifier. We mainly tuned two parameters, `maxDepth` and `maxIter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.215068Z",
     "start_time": "2019-10-15T15:13:18.988Z"
    }
   },
   "outputs": [],
   "source": [
    "GBTClassifier(maxIter=10,seed=42)\n",
    "\n",
    "# build paramGrid\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth,[5, 10]) \\\n",
    "    .addGrid(gbt.maxIter, [10,15]) \\\n",
    "    .build()\n",
    "\n",
    "# set evaluator\n",
    "f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "crossval_gbt = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid_gbt,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.217565Z",
     "start_time": "2019-10-15T15:13:18.993Z"
    }
   },
   "outputs": [],
   "source": [
    "cvModel_gbt = crossval_gbt.fit(train)\n",
    "cvModel_gbt.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model <a id='best_model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.221135Z",
     "start_time": "2019-10-15T15:13:18.998Z"
    }
   },
   "outputs": [],
   "source": [
    "gbt_best = GBTClassifier(maxIter=10,seed=42, maxDepth=5)\n",
    "gbt_best_model = gbt_best.fit(train)\n",
    "results_final = gbt_best_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final result metrics <a id='final_metrics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.223662Z",
     "start_time": "2019-10-15T15:13:19.005Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Test set metrics:')\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_final, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(results_final, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance from GBT model <a id='importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T15:13:19.226604Z",
     "start_time": "2019-10-15T15:13:19.010Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_imp = gbt_best_model.featureImportances.values\n",
    "cols = [\"lifetime\", \"total_songs\", \"num_thumb_up\", \\\n",
    "          'num_thumb_down','add_to_playlist','add_friend','listen_time','avg_songs_played', \\\n",
    "        'gender', 'artist_count']\n",
    "y_pos = np.arange(len(cols))\n",
    " \n",
    "plt.barh(y_pos, feat_imp, align='center')\n",
    "plt.yticks(y_pos, cols)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('GBT Feature Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilized the feature importance attribute of the Gradient Boosted Tree model. From the chart, we can see length of using the service actually plays a very important role. But this may be biased by the fact that those who churned definitely had shorter period of time using the service, and thus, we may re-consider our model or do some transformation to reduce the bias. Other than that, number of thumbs down, number of friend added and number of songs added to the playlist also play important role. For example, large number of thumbs down may indicate that our service cannot correctly recommend songs to user; large number of songs added may indicate that the user loves the song provided by our service, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this notebook, we implemented a model trying to predict customer churn. We removed rows with no userId, converted timestamp to a human readable format, converted gender to binary numeric column. 10 features were built for our model. We selected 4 models: logistic regression, GBM, SVM, and RF to compare and select GBM as the final model implemented for predicting final result. We used cross validation and grid search to fine tune our model. We achieved about 80% accuracy, and 0.81 F1 score, which is about 15% improvement compare to our baseline model - sending everyone an offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "This project gives exposure to spark environment to analyze a large volume of data that a personal laptop may not be capable to analyze. By identifying customer with high chance to churn prior to the acutal losing, companies are able to use minimal cost to save customers by using targeted messages and offers. \n",
    "\n",
    "One of the interesting yet difficult things during the project is brainstroming the features that we can derived from the data we have on hand. Developing useful features is crucial to developing a good model, and requires a lot of energy and efforts. Explanatory and exploratory data analysis play important role in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "The features can be improved a lot after considering more factors, adding more domain knowledges and expertise. Although the volume of data may required tools such as spark to analyze, but we can use more data to have better results as the user base grow.\n",
    "\n",
    "Currently, we have about 450 records of unique users, and we only use 60% of them to train. That said, the model has a huge potential to improve if the sample size increase, and the expected performance will also increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
