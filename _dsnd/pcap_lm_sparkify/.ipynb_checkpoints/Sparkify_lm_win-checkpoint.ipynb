{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T01:26:30.627816Z",
     "start_time": "2019-10-24T01:26:29.266765Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf, count\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T01:26:42.198881Z",
     "start_time": "2019-10-24T01:26:34.718619Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:08:31.108658Z",
     "start_time": "2019-10-24T02:08:22.765521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5157000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>userId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>page</th>\n",
       "      <th>auth</th>\n",
       "      <th>method</th>\n",
       "      <th>status</th>\n",
       "      <th>level</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>location</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>lastName</th>\n",
       "      <th>firstName</th>\n",
       "      <th>registration</th>\n",
       "      <th>gender</th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>PUT</td>\n",
       "      <td>200</td>\n",
       "      <td>paid</td>\n",
       "      <td>50</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>Colin</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>M</td>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>277.89016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ts userId  sessionId      page       auth method  status level  \\\n",
       "0  1538352117000     30         29  NextSong  Logged In    PUT     200  paid   \n",
       "\n",
       "   itemInSession         location  \\\n",
       "0             50  Bakersfield, CA   \n",
       "\n",
       "                                           userAgent lastName firstName  \\\n",
       "0  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  Freeman     Colin   \n",
       "\n",
       "   registration gender          artist       song     length  \n",
       "0  1.538173e+12      M  Martha Tilston  Rockpools  277.89016  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10% smaller\n",
    "## load data\n",
    "import pandas as pd\n",
    "file = 'mini_sparkify_event_data.json.bz2'\n",
    "pddf = pd.read_json(file,compression='bz2')\n",
    "display(pddf.size)\n",
    "pddf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:08:31.201275Z",
     "start_time": "2019-10-24T02:08:31.112645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>userId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>page</th>\n",
       "      <th>auth</th>\n",
       "      <th>method</th>\n",
       "      <th>status</th>\n",
       "      <th>level</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>location</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>lastName</th>\n",
       "      <th>firstName</th>\n",
       "      <th>registration</th>\n",
       "      <th>gender</th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>63292</td>\n",
       "      <td>1539960630000</td>\n",
       "      <td></td>\n",
       "      <td>1050</td>\n",
       "      <td>Home</td>\n",
       "      <td>Logged Out</td>\n",
       "      <td>GET</td>\n",
       "      <td>200</td>\n",
       "      <td>paid</td>\n",
       "      <td>69</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ts userId  sessionId  page        auth method  status level  \\\n",
       "63292  1539960630000              1050  Home  Logged Out    GET     200  paid   \n",
       "\n",
       "       itemInSession location userAgent lastName firstName  registration  \\\n",
       "63292             69     None      None     None      None           NaN   \n",
       "\n",
       "      gender artist  song  length  \n",
       "63292   None   None  None     NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## smaller\n",
    "pddf_small = pddf.sample(frac=0.1)\n",
    "display(pddf_small.size)\n",
    "pddf_small.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:12:26.973481Z",
     "start_time": "2019-10-24T02:12:26.965812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10pre_mini_sparkify_event_data.json.bz2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## new file name\n",
    "newfile = '10pre_' + file\n",
    "newfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:14:34.900128Z",
     "start_time": "2019-10-24T02:14:33.029385Z"
    }
   },
   "outputs": [],
   "source": [
    "## persistence compression\n",
    "pddf_small.to_json(newfile,compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:15:27.128975Z",
     "start_time": "2019-10-24T02:15:26.954272Z"
    }
   },
   "outputs": [],
   "source": [
    "## persistence normal\n",
    "pddf_small.to_json('10pre_mini_sparkify_event_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:47:14.680078Z",
     "start_time": "2019-10-15T01:47:14.677109Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-27414cb1-6f23-43f7-b5c4-068f7ddd28df',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',\n",
    "    'api_key': 'XqTOGm6hBHQPBph0av9FxudyXZZx7phmmVrkAVZN05NE'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_7bc372256f5b42afa24a39e2c2637134_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:47:15.047705Z",
     "start_time": "2019-10-15T01:47:15.044738Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-ldhntltm3yvkv2'))\n",
    "df.take(5)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T01:28:00.881895Z",
     "start_time": "2019-10-24T01:27:59.218920Z"
    }
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nextraneous input '-' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)\\n\\n== SQL ==\\nsparkify-donotdelete-pr-ldhntltm3yvkv2\\n--------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.schema.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input '-' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)\n\n== SQL ==\nsparkify-donotdelete-pr-ldhntltm3yvkv2\n--------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableSchema(ParseDriver.scala:64)\n\tat org.apache.spark.sql.types.StructType$.fromDDL(StructType.scala:449)\n\tat org.apache.spark.sql.DataFrameReader.schema(DataFrameReader.scala:87)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:835)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cab534bc20e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df = spark.read.json('mini_sparkify_event_data.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## 不设定名字貌似最后一步会报错,和ibm有关，drop，研究a'w's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mini_sparkify_event_data.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sparkify-donotdelete-pr-ldhntltm3yvkv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestampFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiLine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mallowUnquotedControlChars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowUnquotedControlChars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             samplingRatio=samplingRatio, dropFieldIfAllNull=dropFieldIfAllNull, encoding=encoding)\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m_set_opts\u001b[0;34m(self, schema, **options)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mschema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schema should be StructType or string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nextraneous input '-' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)\\n\\n== SQL ==\\nsparkify-donotdelete-pr-ldhntltm3yvkv2\\n--------^^^\\n\""
     ]
    }
   ],
   "source": [
    "# 使用压缩格式会死掉，cpu很高但无反应，后续要救\n",
    "## 另外，最好使用 bz2 （gz也行但压缩率没有bz2高） \n",
    "## 这两种格式是 splittble 的，可以使用所有 spark 节点读取\n",
    "#df = spark.read.json('mini_sparkify_event_data.json')\n",
    "## 不设定名字貌似最后一步会报错,和ibm有关，drop，研究a'w's\n",
    "df = spark.read.json('mini_sparkify_event_data.json', 'sparkify-donotdelete-pr-ldhntltm3yvkv2')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:52:44.324164Z",
     "start_time": "2019-10-15T01:52:40.901468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.persist of DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "|100004|\n",
      "|100005|\n",
      "|100006|\n",
      "|100007|\n",
      "|100008|\n",
      "|100009|\n",
      "|100010|\n",
      "|100011|\n",
      "|100012|\n",
      "|100013|\n",
      "|100014|\n",
      "|100015|\n",
      "|100016|\n",
      "|100017|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|      299|\n",
      "|     1059|\n",
      "|      347|\n",
      "|      237|\n",
      "|      241|\n",
      "|       54|\n",
      "|      330|\n",
      "|     1338|\n",
      "|     1217|\n",
      "|      564|\n",
      "|     2263|\n",
      "|      926|\n",
      "|      656|\n",
      "|      270|\n",
      "|      293|\n",
      "|     1532|\n",
      "|     1409|\n",
      "|     1277|\n",
      "|     2088|\n",
      "|       22|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_valid = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "\n",
    "display(df.persist)\n",
    "display(df_valid.select(\"userId\").dropDuplicates().sort(\"userId\").show())\n",
    "display(df_valid.select(\"sessionId\").dropDuplicates().sort(\"userId\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:53:03.671980Z",
     "start_time": "2019-10-15T01:53:03.084686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = df_valid.filter(df[\"userId\"] != \"\")\n",
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:54:00.453967Z",
     "start_time": "2019-10-15T01:53:59.518492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see what values page feature holds\n",
    "df_valid.select(\"page\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:55:41.696313Z",
     "start_time": "2019-10-15T01:55:40.862431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['125', '51', '54', '100014', '101', '29', '100021', '87', '73', '3']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list for churn users\n",
    "## 使用 Cancellation Confirmation 来定义客户流失\n",
    "churn_user_df = df.filter(df.page==\"Cancellation Confirmation\").select(\"userId\").dropDuplicates()\n",
    "churn_user_list = [user[\"userId\"] for user in churn_user_df.collect()]\n",
    "churn_user_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:57:10.243338Z",
     "start_time": "2019-10-15T01:57:10.029885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, churn: boolean]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column for churn\n",
    "df_valid = df_valid.withColumn(\"churn\", df_valid.userId.isin(churn_user_list))\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // group by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:58:46.117421Z",
     "start_time": "2019-10-15T01:58:41.710022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|gender|churn|count|\n",
      "+------+-----+-----+\n",
      "|     M| true|   32|\n",
      "|     F|false|   84|\n",
      "|     F| true|   20|\n",
      "|     M|false|   89|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show gender difference in churn\n",
    "df_valid.dropDuplicates(['userId']).groupby(['gender','churn']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // gender divided played songs stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:00:54.906759Z",
     "start_time": "2019-10-15T02:00:52.515293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_valid_songs = df_valid.where(df_valid.song!='null').groupby(['churn','userId']) \\\n",
    "    .agg(count(df_valid.song).alias('SongsPlayed')).orderBy('churn').toPandas()\n",
    "df_valid_songs.hist(['SongsPlayed'], by='churn', sharex=True, sharey=True,density=1)\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### // gender divided thumps-up/down stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:04:27.275429Z",
     "start_time": "2019-10-15T02:04:24.895252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThumbsUps for people who stay 62.89411764705882\n",
      "ThumbsUps for people who leave 37.18\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# up\n",
    "df_valid_thumbsup = df_valid.where(df_valid.page=='Thumbs Up') \\\n",
    "    .groupby(['churn','userId']).agg(count(col('page')).alias('ThumbsUps')).orderBy('churn').toPandas()\n",
    "df_valid_thumbsup.hist('ThumbsUps', by='churn', bins=20, sharex=True, sharey=True, density=1);\n",
    "\n",
    "print('ThumbsUps for people who stay',df_valid_thumbsup[df_valid_thumbsup['churn']==False]['ThumbsUps'].mean())\n",
    "print('ThumbsUps for people who leave',df_valid_thumbsup[df_valid_thumbsup['churn']==True]['ThumbsUps'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:05:51.206681Z",
     "start_time": "2019-10-15T02:05:48.989303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengf\\Miniconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "C:\\Users\\mengf\\Miniconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThumbsDowns for people who stay 12.735294117647058\n",
      "ThumbsDowns for people who leave 11.545454545454545\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# down\n",
    "df_valid_thumbsdown = df_valid.where(df_valid.page=='Thumbs Down') \\\n",
    "    .groupby(['churn','userId']).agg(count(col('page')).alias('ThumbsDowns')).orderBy('churn').toPandas()\n",
    "df_valid_thumbsdown.hist('ThumbsDowns', by='churn', bins=20, sharex=True, sharey=True, density=1);\n",
    "\n",
    "print('ThumbsDowns for people who stay',df_valid_thumbsdown[df_valid_thumbsup['churn']==False]['ThumbsDowns'].mean())\n",
    "print('ThumbsDowns for people who leave',df_valid_thumbsdown[df_valid_thumbsup['churn']==True]['ThumbsDowns'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:07:01.089505Z",
     "start_time": "2019-10-15T02:06:58.710002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|userId|gender|\n",
      "+------+------+\n",
      "|    10|     0|\n",
      "|   100|     0|\n",
      "|100001|     1|\n",
      "|100002|     1|\n",
      "|100003|     1|\n",
      "|100004|     1|\n",
      "|100005|     0|\n",
      "|100006|     1|\n",
      "|100007|     1|\n",
      "|100008|     1|\n",
      "|100009|     0|\n",
      "|100010|     1|\n",
      "|100011|     0|\n",
      "|100012|     0|\n",
      "|100013|     1|\n",
      "|100014|     0|\n",
      "|100015|     1|\n",
      "|100016|     0|\n",
      "|100017|     0|\n",
      "|100018|     0|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick gender as first feature\n",
    "df_gender = df_valid.dropDuplicates(['userId']).sort('userId').select(['userId','gender'])\n",
    "df_gender = df_gender.replace(['F','M'], ['1', '0'], 'gender')\n",
    "df_gender.withColumn('gender', df_gender.gender.cast(\"int\"))\n",
    "\n",
    "df_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:07:34.534386Z",
     "start_time": "2019-10-15T02:07:32.829764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|userId|SongsPlayed|\n",
      "+------+-----------+\n",
      "|    10|        673|\n",
      "|   100|       2682|\n",
      "|100001|        133|\n",
      "|100002|        195|\n",
      "|100003|         51|\n",
      "|100004|        942|\n",
      "|100005|        154|\n",
      "|100006|         26|\n",
      "|100007|        423|\n",
      "|100008|        772|\n",
      "|100009|        518|\n",
      "|100010|        275|\n",
      "|100011|         11|\n",
      "|100012|        476|\n",
      "|100013|       1131|\n",
      "|100014|        257|\n",
      "|100015|        800|\n",
      "|100016|        530|\n",
      "|100017|         52|\n",
      "|100018|       1002|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick number of songs played as second feature\n",
    "df_songs = df_valid.where(df_valid.song!='null').groupby('userId') \\\n",
    "    .agg(count(df_valid.song).alias('SongsPlayed')).orderBy('userId') \\\n",
    "    .select(['userId','SongsPlayed'])\n",
    "df_songs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:15:53.657008Z",
     "start_time": "2019-10-15T02:15:50.846741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|userId|ThumbsUps|\n",
      "+------+---------+\n",
      "|    10|       37|\n",
      "|   100|      148|\n",
      "|100001|        8|\n",
      "|100002|        5|\n",
      "|100003|        3|\n",
      "|100004|       35|\n",
      "|100005|        7|\n",
      "|100006|        2|\n",
      "|100007|       19|\n",
      "|100008|       37|\n",
      "|100009|       23|\n",
      "|100010|       17|\n",
      "|100012|       18|\n",
      "|100013|       39|\n",
      "|100014|       17|\n",
      "|100015|       35|\n",
      "|100016|       25|\n",
      "|100017|        2|\n",
      "|100018|       46|\n",
      "|100019|        1|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----------+\n",
      "|userId|ThumbsDowns|\n",
      "+------+-----------+\n",
      "|    10|          4|\n",
      "|   100|         27|\n",
      "|100001|          2|\n",
      "|100004|         11|\n",
      "|100005|          3|\n",
      "|100006|          2|\n",
      "|100007|          6|\n",
      "|100008|          6|\n",
      "|100009|          8|\n",
      "|100010|          5|\n",
      "|100011|          1|\n",
      "|100012|          9|\n",
      "|100013|         15|\n",
      "|100014|          3|\n",
      "|100015|          8|\n",
      "|100016|          5|\n",
      "|100017|          1|\n",
      "|100018|          9|\n",
      "|100019|          1|\n",
      "|100021|          5|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick number of thumbs up and thumps down as third and fourth feature\n",
    "df_thumbsup = df_valid.where(df_valid.page=='Thumbs Up') \\\n",
    "    .groupby(['userId']).agg(count(col('page')).alias('ThumbsUps')).orderBy('userId').select(['userId','ThumbsUps'])\n",
    "df_thumbsdown = df_valid.where(df_valid.page=='Thumbs Down') \\\n",
    "    .groupby(['userId']).agg(count(col('page')).alias('ThumbsDowns')).orderBy('userId').select(['userId','ThumbsDowns'])\n",
    "df_thumbsup.show(),df_thumbsdown.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:17:06.135824Z",
     "start_time": "2019-10-15T02:16:28.575125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|SingersListened|\n",
      "+------+---------------+\n",
      "|    10|            565|\n",
      "|   100|           1705|\n",
      "|100001|            125|\n",
      "|100002|            184|\n",
      "|100003|             50|\n",
      "|100004|            733|\n",
      "|100005|            149|\n",
      "|100006|             26|\n",
      "|100007|            357|\n",
      "|100008|            623|\n",
      "|100009|            442|\n",
      "|100010|            252|\n",
      "|100011|             11|\n",
      "|100012|            397|\n",
      "|100013|            826|\n",
      "|100014|            233|\n",
      "|100015|            627|\n",
      "|100016|            431|\n",
      "|100017|             51|\n",
      "|100018|            780|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick the number of singers user has listened as fifth feature\n",
    "df_singers = df_valid.dropDuplicates(['userId','artist']).groupby('userId') \\\n",
    "    .agg(count(df_valid.artist).alias('SingersListened')).orderBy('userId') \\\n",
    "    .select(['userId','SingersListened'])\n",
    "df_singers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:18:37.326913Z",
     "start_time": "2019-10-15T02:17:14.358447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----------+---------+-----------+---------------+\n",
      "|userId|churn|gender|SongsPlayed|ThumbsUps|ThumbsDowns|SingersListened|\n",
      "+------+-----+------+-----------+---------+-----------+---------------+\n",
      "|100010|false|     1|        275|       17|          5|            252|\n",
      "|200002|false|     0|        387|       21|          6|            339|\n",
      "|   124|false|     1|       4079|      171|         41|           2232|\n",
      "|    51| true|     0|       2111|      100|         21|           1385|\n",
      "|     7|false|     0|        150|        7|          1|            142|\n",
      "|    15|false|     0|       1914|       81|         14|           1302|\n",
      "|    54| true|     1|       2841|      163|         29|           1744|\n",
      "|   155|false|     1|        820|       58|          3|            643|\n",
      "|100014| true|     0|        257|       17|          3|            233|\n",
      "|   132|false|     1|       1928|       96|         17|           1299|\n",
      "|   101| true|     0|       1797|       86|         16|           1241|\n",
      "|    11|false|     1|        647|       40|          9|            534|\n",
      "|   138|false|     0|       2070|       95|         24|           1332|\n",
      "|300017|false|     1|       3632|      303|         28|           2070|\n",
      "|100021| true|     0|        230|       11|          5|            207|\n",
      "|    29| true|     0|       3028|      154|         22|           1804|\n",
      "|    69|false|     1|       1125|       72|          9|            865|\n",
      "|   112|false|     0|        215|        9|          3|            195|\n",
      "|    42|false|     1|       3573|      166|         25|           2073|\n",
      "|    73| true|     1|        377|       14|          7|            334|\n",
      "+------+-----+------+-----------+---------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join features\n",
    "df_final = df_valid.dropDuplicates(['userId']).sort('userId').select(['userId','churn'])\n",
    "for feature in [df_gender, df_songs,df_thumbsup,df_thumbsdown,df_singers]:\n",
    "    df_final = df_final.join(feature,'userId')\n",
    "    \n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:21:20.634675Z",
     "start_time": "2019-10-15T02:21:20.629689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'string'),\n",
       " ('churn', 'boolean'),\n",
       " ('gender', 'string'),\n",
       " ('SongsPlayed', 'bigint'),\n",
       " ('ThumbsUps', 'bigint'),\n",
       " ('ThumbsDowns', 'bigint'),\n",
       " ('SingersListened', 'bigint')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:21:56.039619Z",
     "start_time": "2019-10-15T02:21:56.005710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'string'),\n",
       " ('churn', 'float'),\n",
       " ('gender', 'float'),\n",
       " ('SongsPlayed', 'float'),\n",
       " ('ThumbsUps', 'float'),\n",
       " ('ThumbsDowns', 'float'),\n",
       " ('SingersListened', 'float')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data type of columns into float\n",
    "for feature in df_final.columns[1:]:\n",
    "    df_final = df_final.withColumn(feature,df_final[feature].cast('float'))\n",
    "    \n",
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:25:11.600807Z",
     "start_time": "2019-10-15T02:23:47.038550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----------+---------+-----------+---------------+\n",
      "|userId|churn|gender|SongsPlayed|ThumbsUps|ThumbsDowns|SingersListened|\n",
      "+------+-----+------+-----------+---------+-----------+---------------+\n",
      "|100010|  0.0|   1.0|      275.0|     17.0|        5.0|          252.0|\n",
      "|200002|  0.0|   0.0|      387.0|     21.0|        6.0|          339.0|\n",
      "|   124|  0.0|   1.0|     4079.0|    171.0|       41.0|         2232.0|\n",
      "|    51|  1.0|   0.0|     2111.0|    100.0|       21.0|         1385.0|\n",
      "|     7|  0.0|   0.0|      150.0|      7.0|        1.0|          142.0|\n",
      "+------+-----+------+-----------+---------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特别慢！ spark 在mac 和windows经常这样，需要研究下\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:35:29.095693Z",
     "start_time": "2019-10-15T02:35:29.064292Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train, test = df_final.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:54:16.669195Z",
     "start_time": "2019-10-15T02:36:54.534940Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5188.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4468.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4468.0 (TID 95899, localhost, executor driver): java.io.FileNotFoundException: File file:/C:/Users/mengf/Documents/GitHub/udacity-private/_dsnd/p4_capestone/project_sparkify/mini_sparkify_event_data.json does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.SampleExec.inputRDDs(basicPhysicalOperators.scala:271)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3043)\r\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3041)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionSummary$class.$init$(LogisticRegression.scala:1400)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionSummaryImpl.<init>(LogisticRegression.scala:1655)\r\n\tat org.apache.spark.ml.classification.BinaryLogisticRegressionSummaryImpl.<init>(LogisticRegression.scala:1697)\r\n\tat org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl.<init>(LogisticRegression.scala:1676)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:893)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/mengf/Documents/GitHub/udacity-private/_dsnd/p4_capestone/project_sparkify/mini_sparkify_event_data.json does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-5cc5f17df169>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mcvModel_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossval_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0mbestIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mbestModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCrossValidatorModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubModels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py37\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5188.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4468.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4468.0 (TID 95899, localhost, executor driver): java.io.FileNotFoundException: File file:/C:/Users/mengf/Documents/GitHub/udacity-private/_dsnd/p4_capestone/project_sparkify/mini_sparkify_event_data.json does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.SampleExec.inputRDDs(basicPhysicalOperators.scala:271)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3043)\r\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3041)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionSummary$class.$init$(LogisticRegression.scala:1400)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionSummaryImpl.<init>(LogisticRegression.scala:1655)\r\n\tat org.apache.spark.ml.classification.BinaryLogisticRegressionSummaryImpl.<init>(LogisticRegression.scala:1697)\r\n\tat org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl.<init>(LogisticRegression.scala:1676)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:893)\r\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/mengf/Documents/GitHub/udacity-private/_dsnd/p4_capestone/project_sparkify/mini_sparkify_event_data.json does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "assembler = VectorAssembler(inputCols=df_final.columns[2:],outputCol='featuresassemble')\n",
    "scaler = StandardScaler(inputCol=\"featuresassemble\", outputCol=\"features\", withStd=True)\n",
    "indexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "\n",
    "lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"indexed\", seed=42)\n",
    "\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, indexer, lr])\n",
    "\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "\n",
    "cvModel_lr = crossval_lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T02:54:16.671190Z",
     "start_time": "2019-10-15T02:37:01.900Z"
    }
   },
   "outputs": [],
   "source": [
    "cvModel_lr.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
