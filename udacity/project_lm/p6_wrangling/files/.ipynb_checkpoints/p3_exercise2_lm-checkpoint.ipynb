{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# /5评估数据/ Assessing\n",
    "Use the space below to explore `all_alpha_08.csv` and `all_alpha_18.csv` to answer the quiz questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df08 = pd.read_csv('all_alpha_08.txt', sep = '\\t')\n",
    "df18 = pd.read_csv('all_alpha_18.txt', sep = '\\t')\n",
    "# 注意原文件链接为xlsx、zip和txt（不是csv，csv课程中没给出链接）\n",
    "# 使用read_csv读入，发现没有分列，看样子是tab分割\n",
    "# 加个 sep = '\\t'指定使用tab分割解决\n",
    "# （mac 遇到奇怪问题下载的文件都很奇怪，可能和单位网有关）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.每个数据集中的样本数\n",
    "\n",
    "2.每个数据集中的列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df08.shape)\n",
    "print(df18.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.每个数据集中重复的行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dup_08 = sum(df08.duplicated())\n",
    "sum_dup_18 = sum(df18.duplicated())\n",
    "print(sum_dup_08,sum_dup_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.列的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.具有缺失值的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list = []\n",
    "for i in df08.columns:\n",
    "    if df08[i].isnull().any():\n",
    "        null_list.append(i)\n",
    "null_list\n",
    "# 使用判断把空置输出为一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用isnull.sum检查数量\n",
    "nucheck = df08.isnull().sum()\n",
    "nucheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看下占的百分比\n",
    "round(nucheck/df08.shape[0],3)\n",
    "# round(float,n) 的作用是把float小数，保留到小数点后n位\n",
    "# 当然了 nucheck是求出的丢失值的数量，df08.shape[0]是所有项，案例1有讲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.每个数据集中特征的非空唯一值的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df08.columns:\n",
    "    print(i, end = ' : ')\n",
    "    print(df08[i].nunique())\n",
    "# 可以使用格式化字符串来更好的显示，略过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些唯一值都是什么，以及每个的计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上value_counts 看下（车型较多有400多种）\n",
    "# 使用[1:]将车型过滤掉\n",
    "for i in df08.columns[1:]:\n",
    "    print('\\n', i, ':')\n",
    "    print(df08[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /6清理列标签/ 清除列标签\n",
    "使用 all_alpha_08.csv 和 all_alpha_18.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先检查下原有数据\n",
    "print(df08.shape)\n",
    "df08.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 丢弃多余列\n",
    "drop_list = ['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG']\n",
    "df08.drop(drop_list, axis=1, inplace=True)\n",
    "print(df08.shape)\n",
    "df08.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Stnd','Stnd Description','Underhood ID','Comb CO2']\n",
    "df18.drop(drop_list, axis=1, inplace=True)\n",
    "print(df18.shape)\n",
    "df18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先检查一下有没有列不同的\n",
    "df08.columns == df18.columns\n",
    "# 输出是个boolean的列表，比较直观"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把08列的名字得出来\n",
    "for i in df08.columns:\n",
    "    if i not in df18.columns:\n",
    "        print(i)\n",
    "\n",
    "# 把18列的名字得出来\n",
    "for i in df18.columns:\n",
    "    if i not in df08.columns:\n",
    "        print(i)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将08的替换为18的\n",
    "df08.rename(columns = \n",
    "            lambda x: x.replace('Sales Area', 'Cert Region'), \n",
    "            inplace = True)\n",
    "# 这里使用的是lambda x:\n",
    "# 就是对columns执行后面的操作 x.replace\n",
    "# 而x.replace的操作是把后面小括弧中的前面替换成后面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还有就是把列中的空格替换成下滑线\n",
    "# 因为很多处理要对空格判断，不要有比较好\n",
    "# 全部变为小写是习惯，了解一下吧\n",
    "df08.rename(columns=lambda x:\n",
    "             x.strip().lower().replace(\" \", \"_\"),\n",
    "             inplace=True)\n",
    "# .strip是去除单词首尾的空格\n",
    "# .lower是变为小写\n",
    "# .replace是把小括弧里的做替换\n",
    "df08.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df18 也做一遍，再检查列是否一致\n",
    "df18.rename(columns=lambda x:\n",
    "             x.strip().lower().replace(\" \", \"_\"),\n",
    "             inplace=True)\n",
    "print(df18.columns)\n",
    "df08.columns == df18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以用这个，比较安静的，因为加了.all()所以只会出一个True或者False\n",
    "(df08.columns == df18.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "df08.to_csv('data_08.csv', index=False)\n",
    "df18.to_csv('data_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /7过滤、丢空、去重/ 过滤、丢弃空值、重复数据删除\n",
    "使用 data_08.csv 和 data_18.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按认证区域过滤09\n",
    "def cleancol(df):\n",
    "    print(df.shape)\n",
    "    df['cert_region'].unique()\n",
    "    df = df.query('cert_region == \"CA\"')\n",
    "    print(df.shape)      \n",
    "    print(df['cert_region'].unique())\n",
    "    df.drop(['cert_region'],axis=1,inplace=True)\n",
    "    print(df.shape) \n",
    "    return df.info()\n",
    "\n",
    "\n",
    "cleancol(df18)\n",
    "cleancol(df08)\n",
    "# 此cell如果执行多遍会有问题因为已经drop了cert_region\n",
    "# 少年，要不要使用try/except优化了？\n",
    "# 另外后面有个报错，怎么优化呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 丢弃含有缺失值的行\n",
    "# 检查下缺失值\n",
    "print(df08.isnull().sum())\n",
    "# 丢弃有缺失的\n",
    "df08.dropna(inplace = True)\n",
    "df08.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df18.isnull().sum())\n",
    "df18.dropna(inplace = True)\n",
    "df18.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check 一下\n",
    "df08.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复数据删除\n",
    "print(df08.duplicated().sum())\n",
    "df08.drop_duplicates(inplace = True)\n",
    "print(df08.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df18.duplicated().sum())\n",
    "df18.drop_duplicates(inplace = True)\n",
    "print(df18.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存进度，以便下一段使用\n",
    "df08.to_csv('data_08b.csv', index=False)\n",
    "df18.to_csv('data_18b.csv', index=False)\n",
    "# 建议使用不同的文件名，等都完成后再调优，避免覆盖文件带来的麻烦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /8检查数据类型/ Inspecting Data Types\n",
    "Use the space below to explore data_08.csv and data_18.csv to answer the quiz questions below regarding datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df08.head(1))\n",
    "type(df08.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cyl字符串要转换成数字\n",
    "- air_polution转成浮点（object是字符串）\n",
    "- city_mpg、hwy_mpg、cmb_mpg将字符串转换为浮点型\n",
    "- greenhouse_gas_score从字符转为浮点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /9修正数据类型第1部分/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用str的extract检索里面的数字\n",
    "print(df08['cyl'].value_counts())\n",
    "df08['cyl'] = df08['cyl'].str.extract('(\\d+)').astype(int)\n",
    "# （\\d+)是匹配任意数字的意思，有兴趣的可以看看reg或者回顾week2格式化字符串\n",
    "print(df08['cyl'].value_counts())\n",
    "type(df08['cyl'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyl18\n",
    "print(df18['cyl'].value_counts())\n",
    "df18['cyl'] = df18['cyl'].astype(int)\n",
    "# 貌似astype不能使用inplace参数\n",
    "print(df18['cyl'].value_counts())\n",
    "type(df18['cyl'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /10修正数据类型第2部分/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# air08\n",
    "type(df08['air_pollution_score'][0])\n",
    "df08['air_pollution_score'].astype(float, inplace = True)\n",
    "df08['air_pollution_score'].astype(int, inplace = True)\n",
    "# 像‘5.0’这样的字符如果使用int转换会报错\n",
    "# ValueError: invalid literal for int() with base 10:\n",
    "# 但使用float转换不会报错，详见\n",
    "# https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10#\n",
    "type(df08['air_pollution_score'][0])\n",
    "# 因为有 x/y 这种，所以转换会报错\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出含有 / 的部分\n",
    "# 也可以使用query方式\n",
    "hb_08 = df08[df08['fuel'].str.contains('/')]\n",
    "print(hb_08.shape)\n",
    "\n",
    "# 拆分为两列，使用copy否则会修改原数据\n",
    "# create two copies of the 2008 hybrids dataframe\n",
    "df1 = hb_08.copy()  # data on first fuel type of each hybrid vehicle\n",
    "df2 = hb_08.copy()  # data on second fuel type of each hybrid vehicle\n",
    "\n",
    "# 确定要拆分的列\n",
    "# columns to split by \"/\"\n",
    "split_columns = ['fuel', 'air_pollution_score', 'city_mpg', 'hwy_mpg', 'cmb_mpg', 'greenhouse_gas_score']\n",
    "\n",
    "# apply split function to each column of each dataframe copy\n",
    "for c in split_columns:\n",
    "    df1[c] = df1[c].apply(lambda x: x.split(\"/\")[0])\n",
    "    df2[c] = df2[c].apply(lambda x: x.split(\"/\")[1])\n",
    "    # lambda对每个带/的进行分割，分别赋值第1个和第2个拆分元素\n",
    "print(df1.head(3))\n",
    "# 看下是否成功\n",
    "\n",
    "dfnewrows = df1.append(df2)\n",
    "print(df1.shape)\n",
    "print(dfnewrows.shape)\n",
    "dfnewrows.head(3)\n",
    "# 检查数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新df08\n",
    "print(df08.shape)\n",
    "print(hb_08.index)\n",
    "# 检查下要删除的行\n",
    "df08.drop(hb_08.index, inplace = True)\n",
    "df08 = df08.append(dfnewrows, ignore_index = True)\n",
    "# append 没有inplace参数\n",
    "df08.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018的数据\n",
    "# air 和 green已经是float格式不用拆分\n",
    "# 找出含有 / 的部分\n",
    "# 也可以使用query方式\n",
    "hb_18 = df18[df18['fuel'].str.contains('/')]\n",
    "print(hb_18.shape)\n",
    "\n",
    "# 拆分为两列，使用copy否则会修改原数据\n",
    "# create two copies of the 2008 hybrids dataframe\n",
    "df1 = hb_18.copy()  # data on first fuel type of each hybrid vehicle\n",
    "df2 = hb_18.copy()  # data on second fuel type of each hybrid vehicle\n",
    "\n",
    "# 确定要拆分的列\n",
    "# columns to split by \"/\"\n",
    "split_columns = ['fuel', 'city_mpg', 'hwy_mpg', 'cmb_mpg']\n",
    "\n",
    "# apply split function to each column of each dataframe copy\n",
    "for c in split_columns:\n",
    "    df1[c] = df1[c].apply(lambda x: x.split(\"/\")[0])\n",
    "    df2[c] = df2[c].apply(lambda x: x.split(\"/\")[1])\n",
    "    # lambda对每个带/的进行分割，分别赋值第1个和第2个拆分元素\n",
    "print(df1.head(3))\n",
    "# 看下是否成功\n",
    "\n",
    "dfnewrows = df1.append(df2)\n",
    "print(df1.shape)\n",
    "print(dfnewrows.shape)\n",
    "dfnewrows.head(3)\n",
    "# 检查数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新df18\n",
    "print(df18.shape)\n",
    "print(hb_18.index)\n",
    "# 检查下要删除的行\n",
    "df18.drop(hb_18.index, inplace = True)\n",
    "df18 = df18.append(dfnewrows, ignore_index = True)\n",
    "# append 没有inplace参数\n",
    "df18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在可以转换了 08 / 18\n",
    "df08.air_pollution_score = df08.air_pollution_score.astype(float)\n",
    "df18.air_pollution_score = df18.air_pollution_score.astype(float)\n",
    "\n",
    "df08.to_csv('data_08c.csv', index=False)\n",
    "df18.to_csv('data_18c.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /11 修正数据类型第3部分/ 修正 city_mpg、hwy_mpg、cmb_mpg 数据类型\n",
    "2008 和 2018：将字符串转换成浮点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 mpg 列转换成浮点\n",
    "mpg_columns = ['city_mpg','hwy_mpg','cmb_mpg']\n",
    "for c in mpg_columns:\n",
    "    df18[c] = df18[c].astype(float)\n",
    "    df08[c] = df18[c].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 greenhouse转成浮点\n",
    "df18['greenhouse_gas_score'] = df18['greenhouse_gas_score'].astype(float)\n",
    "df08['greenhouse_gas_score'] = df08['greenhouse_gas_score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18.dtypes\n",
    "df18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08.to_csv('clean_08.csv', index=False)\n",
    "df18.to_csv('clean_18.csv', index=False)\n",
    "df18.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /12 使用可视化探索数据/ Exploring with Visuals\n",
    "Use clean_08.csv and clean_18.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p08=plt.hist(df08['greenhouse_gas_score'],color='r', alpha = 0.5)\n",
    "p18=plt.hist(df18['greenhouse_gas_score'],color='b', alpha = 0.5)\n",
    "plt.show()\n",
    "# 哇偶，可以看出18年的greenhouse_gas_score有所下降！\n",
    "# 企鹅宝宝，，，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=plt.hist(df08['city_mpg'],color='r', alpha = 0.5)\n",
    "hwy=plt.hist(df08['hwy_mpg'],color='b', alpha = 0.5)\n",
    "cmb = plt.hist(df08['cmb_mpg'],color='g', alpha = 0.5)\n",
    "plt.show()\n",
    "# 从数据可以推断出city是在城市的速度（miles per hour）\n",
    "# hwy是高速的，cmb是联合的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p08=plt.hist(df08['air_pollution_score'],color='b')\n",
    "p18=plt.hist(df18['air_pollution_score'],color='r')\n",
    "\n",
    "# 也是下降了，不好看\n",
    "# 这里没有家alpha，没有透明度，一下就记住了对不对"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS：数据的链接地址有所有数据的联合csv下载，有兴趣的来啊！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /13 结论和可视展示/ 得出结论\n",
    "使用下列空间来处理数据集clean_08.csv 和clean_18.csv中的以下问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: 与2008年相比，2018年是否有更多的车型（去重后）使用替代能源？比例增长了多少？\n",
    "\n",
    "- 这部分feature pdf中没有特殊说明，bing出来：\n",
    "- CNG 压缩天然气\n",
    "- dissel 柴油\n",
    "- Gasoline 汽油\n",
    "- ethanol 乙醇\n",
    "- gas 天然气\n",
    "\n",
    "瞎想的（可以忽悠不懂的）\n",
    "1. CNG是压缩天然气，另外一种是液化天然气LNG（C代表compressed，L代表liquid，LNG可以制造CNG，NG是不经压缩的gas）。后者更环保因为是液态，行驶也更远。但是前一种更加普遍（因为可以使用现在的汽油汽车改装，难道混合动力的都是这个货？）\n",
    "1. 乙醇更环保,天燃气是混合物,里面可能包含一些S,N这些元素.当这些元素燃烧时会产生污染气体.乙醇燃烧的产物是二氧化碳和水.再者乙醇是可再生资源,所以目前有很多国家在汽油用填加乙醇,已减少汽油的使用量。\n",
    "1. 在18年的数据中还有电力汽车（我的特斯拉啊，看着就帅！，，，，这模型还挺重的）马克思，，哦不马斯克的Solar City了解一下。\n",
    "1. 综上所述，由于各年的value不同，我们就分为两个阵营进行比较（谁说要加权来的，你出来，我保证，，，不打死你，，，）Gasoline和diesel作为传统能源，其他都粗暴的归为清洁能源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df08.groupby(['fuel']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df18.groupby(['fuel']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlist = ['cng', 'ethanol', 'gas', 'electricity']\n",
    "# 先制定清洁能源的备选\n",
    "\n",
    "def cleanratio(df):\n",
    "    cnumber = 0\n",
    "    fuel_list = list(set(df['fuel'].values))\n",
    "    \n",
    "    for i in fuel_list:\n",
    "        if i.lower() in cleanlist:\n",
    "        # 本例子中i不要变化，因为后面还要根据i来匹配\n",
    "            cnumber = cnumber + df[df['fuel'] == i].shape[0]\n",
    "    return round(cnumber / df.shape[0],4)\n",
    "    # 使用round控制小数的精度：round（你要显示的数，你要显示的小数位数）\n",
    "            \n",
    "print(cleanratio(df08))\n",
    "print(cleanratio(df18))\n",
    "# 可见从6%增长到了9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: 各车辆类别（veh_class）在燃料经济性方面的改进（mpg 的增长）是多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过观察，发现cmb在city和hwy之间，应该是经过加权了的，直接比较他\n",
    "mpgc08 = df08.groupby(['veh_class'])['cmb_mpg'].mean()\n",
    "# 其实还可以发现value的种类不同，有兴趣规整一下不少年？\n",
    "mpgc08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpgc18 = df18.groupby(['veh_class'])['cmb_mpg'].mean()\n",
    "mpgc18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为名字是不同的，直接相减\n",
    "mpcompare = mpgc18- mpgc08\n",
    "# 凡事带有NaN的说明在另一个数据里面没有\n",
    "# 不同名字的不比较，使用dropna处理\n",
    "mpcompare.dropna(inplace = True)\n",
    "mpcompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpcompare.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK 可以画图鸟\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "colors = []\n",
    "for value in mpcompare.values:\n",
    "    if value >= 0:\n",
    "        colors.append('g')\n",
    "    else:\n",
    "        colors.append('r')\n",
    "mpcompare.plot(kind = 'bar', alpha = 0.7, color = colors)\n",
    "# color可以通过输入一个列表指定\n",
    "# 要想根据条件输出不同color，写个循环先建立好color列表\n",
    "ax.set_xticklabels(mpcompare.index, fontsize = 12, rotation = 30);\n",
    "ax.set_ylabel('Change MPG', fontsize = 16)\n",
    "ax.set_xlabel('Veh Classed', fontsize = 16)\n",
    "fig.suptitle('Veh MPG Change form 2008 to 2018', fontsize = 24, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: SmartWay 车辆的特点是什么？ 它们是否随着时间而改变？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smartway labels for 2008\n",
    "df08.smartway.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all smartway vehicles in 2008\n",
    "smart_08 = df08.query('smartway == \"yes\"')\n",
    "print(smart_08.info())\n",
    "smart_08.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df18.smartway.unique())\n",
    "smart_18 = df18.query('smartway == \"Yes\" or smartway == \"Elite\"')\n",
    "print(smart_18.info())\n",
    "smart_18.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: 哪些功能与更好的燃油经济性有关？What features are associated with better fuel economy?\n",
    "You can explore trends between cmb_mpg and the other features in this dataset, or filter this dataset like in the previous question and explore the properties of that dataset. For example, you can select all vehicles that have the top 50% fuel economy ratings like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里课程给出的答案好奇怪，等我想想的\n",
    "top_08 = df08.query('cmb_mpg > cmb_mpg.mean()')\n",
    "top_08.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_18 = df18.query('cmb_mpg > cmb_mpg.mean()')\n",
    "top_18.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后一个问题！对于 2008 年生产且 2018 年仍在生产中的车型，mpg 有多少改进，哪些车辆的改进最多？\n",
    "这个问题是关于 2008 年以来不断更新换代且 2018 年仍在生产的车型。要回答此问题，我们需要找到两个数据集中都存在的车型，那么先让我们来了解如何将两个数据集合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df18.shape)\n",
    "df18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df08.shape)\n",
    "print(df08.columns)\n",
    "df08.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要使用merge将2个数据融合\n",
    "# 就能根据mode有2个以上的value count来判断是否两个都有了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df08.rename(\n",
    "    columns = lambda x: x[:10] + '_2008', inplace = True)\n",
    "df08.rename(columns={'model_2008':'model'},inplace=True)\n",
    "len(df08.columns)\n",
    "df08.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么要有个一样的model呢，因为合并时候要有一列相同才好合并么\n",
    "# merge的说明\n",
    "df_combined = pd.merge(df08,df18,on=['model'])\n",
    "df_combined.shape\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg2018 = df_combined.groupby('model')['city_mpg'].mean()\n",
    "mpg2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg2008 = df_combined.groupby('model')['city_mpg_2008'].mean()\n",
    "mpg2008[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个数据都有了，那么我们画图好了吧\n",
    "# 太多了！根据题目，需要研究的是那些改动最多\n",
    "# 那么先计算一下吧\n",
    "mpgchange = mpg2018 - mpg2008\n",
    "# 我们来排序一下(还有sort_index)\n",
    "top10 = round(mpgchange.sort_values()[:10],2)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们来检查下数据\n",
    "print(mpgchange['JEEP Wrangler'])\n",
    "print(mpg2008['JEEP Wrangler'])\n",
    "print(mpg2018['JEEP Wrangler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last10 = round(mpgchange.sort_values(ascending = False)[:10],2)\n",
    "last10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来就可以吧top10和last10画图了，此处自己动手试试吧！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
